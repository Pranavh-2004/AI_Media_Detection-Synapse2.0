{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pranavh-2004/AI_Media_Detection-Synapse2.0/blob/main/Image_hashing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwvZYLue60LO",
        "outputId": "64469013-64a3-4238-9570-d88c865149fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from imagehash) (1.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imagehash) (1.25.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imagehash) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imagehash) (1.11.4)\n",
            "Installing collected packages: imagehash\n",
            "Successfully installed imagehash-4.3.1\n"
          ]
        }
      ],
      "source": [
        "pip install imagehash"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import imagehash\n",
        "hash = imagehash.average_hash(Image.open('tests/data/imagehash.png'))\n",
        "print(hash)\n",
        "otherhash = imagehash.average_hash(Image.open('tests/data/peppers.png'))\n",
        "print(otherhash)\n",
        "print(hash == otherhash)\n",
        "print(hash - otherhash)  # hamming distance"
      ],
      "metadata": {
        "id": "cSPcBmew68H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Perceptual image hashing library\n",
        "\n",
        "Example:\n",
        "\n",
        ">>> from PIL import Image\n",
        ">>> import imagehash\n",
        ">>> hash = imagehash.average_hash(Image.open('test.png'))\n",
        ">>> print(hash)\n",
        "d879f8f89b1bbf\n",
        ">>> otherhash = imagehash.average_hash(Image.open('other.bmp'))\n",
        ">>> print(otherhash)\n",
        "ffff3720200ffff\n",
        ">>> print(hash == otherhash)\n",
        "False\n",
        ">>> print(hash - otherhash)\n",
        "36\n",
        ">>> for r in range(1, 30, 5):\n",
        "...\trothash = imagehash.average_hash(Image.open('test.png').rotate(r))\n",
        "...\tprint('Rotation by %d: %d Hamming difference' % (r, hash - rothash))\n",
        "...\n",
        "Rotation by 1: 2 Hamming difference\n",
        "Rotation by 6: 11 Hamming difference\n",
        "Rotation by 11: 13 Hamming difference\n",
        "Rotation by 16: 17 Hamming difference\n",
        "Rotation by 21: 19 Hamming difference\n",
        "Rotation by 26: 21 Hamming difference\n",
        ">>>\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy\n",
        "from PIL import Image, ImageFilter\n",
        "try:\n",
        "\tANTIALIAS = Image.Resampling.LANCZOS\n",
        "except AttributeError:\n",
        "\t# deprecated in pillow 10\n",
        "\t# https://pillow.readthedocs.io/en/stable/deprecations.html\n",
        "\tANTIALIAS = Image.ANTIALIAS\n",
        "\n",
        "__version__ = '4.3.1'\n",
        "\n",
        "\"\"\"\n",
        "You may copy this file, if you keep the copyright information below:\n",
        "\n",
        "\n",
        "Copyright (c) 2013-2022, Johannes Buchner\n",
        "https://github.com/JohannesBuchner/imagehash\n",
        "\n",
        "All rights reserved.\n",
        "\n",
        "Redistribution and use in source and binary forms, with or without\n",
        "modification, are permitted provided that the following conditions are\n",
        "met:\n",
        "\n",
        "Redistributions of source code must retain the above copyright\n",
        "notice, this list of conditions and the following disclaimer.\n",
        "\n",
        "Redistributions in binary form must reproduce the above copyright\n",
        "notice, this list of conditions and the following disclaimer in the\n",
        "documentation and/or other materials provided with the distribution.\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\n",
        "IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\n",
        "TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\n",
        "PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n",
        "HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n",
        "SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n",
        "LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n",
        "DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n",
        "THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        "(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def _binary_array_to_hex(arr):\n",
        "\t\"\"\"\n",
        "\tinternal function to make a hex string out of a binary array.\n",
        "\t\"\"\"\n",
        "\tbit_string = ''.join(str(b) for b in 1 * arr.flatten())\n",
        "\twidth = int(numpy.ceil(len(bit_string) / 4))\n",
        "\treturn '{:0>{width}x}'.format(int(bit_string, 2), width=width)\n",
        "\n",
        "\n",
        "class ImageHash:\n",
        "\t\"\"\"\n",
        "\tHash encapsulation. Can be used for dictionary keys and comparisons.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, binary_array):\n",
        "\t\t# type: (NDArray) -> None\n",
        "\t\tself.hash = binary_array  # type: NDArray\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\treturn _binary_array_to_hex(self.hash.flatten())\n",
        "\n",
        "\tdef __repr__(self):\n",
        "\t\treturn repr(self.hash)\n",
        "\n",
        "\tdef __sub__(self, other):\n",
        "\t\t# type: (ImageHash) -> int\n",
        "\t\tif other is None:\n",
        "\t\t\traise TypeError('Other hash must not be None.')\n",
        "\t\tif self.hash.size != other.hash.size:\n",
        "\t\t\traise TypeError('ImageHashes must be of the same shape.', self.hash.shape, other.hash.shape)\n",
        "\t\treturn numpy.count_nonzero(self.hash.flatten() != other.hash.flatten())\n",
        "\n",
        "\tdef __eq__(self, other):\n",
        "\t\t# type: (object) -> bool\n",
        "\t\tif other is None:\n",
        "\t\t\treturn False\n",
        "\t\treturn numpy.array_equal(self.hash.flatten(), other.hash.flatten())  # type: ignore\n",
        "\n",
        "\tdef __ne__(self, other):\n",
        "\t\t# type: (object) -> bool\n",
        "\t\tif other is None:\n",
        "\t\t\treturn False\n",
        "\t\treturn not numpy.array_equal(self.hash.flatten(), other.hash.flatten())  # type: ignore\n",
        "\n",
        "\tdef __hash__(self):\n",
        "\t\t# this returns a 8 bit integer, intentionally shortening the information\n",
        "\t\treturn sum([2**(i % 8) for i, v in enumerate(self.hash.flatten()) if v])\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\t# Returns the bit length of the hash\n",
        "\t\treturn self.hash.size\n",
        "\n",
        "\n",
        "# dynamic code for typing\n",
        "try:\n",
        "\t# specify allowed values if possible (py3.8+)\n",
        "\tfrom typing import Literal\n",
        "\tWhashMode = Literal['haar', 'db4']  # type: ignore\n",
        "except ImportError:\n",
        "\tWhashMode = str  # type: ignore\n",
        "\n",
        "try:\n",
        "\t# enable numpy array typing (py3.7+)\n",
        "\timport numpy.typing\n",
        "\tNDArray = numpy.typing.NDArray[numpy.bool_]\n",
        "except (AttributeError, ImportError):\n",
        "\tNDArray = list  # type: ignore\n",
        "\n",
        "# type of Callable\n",
        "if sys.version_info >= (3, 3):\n",
        "\tif sys.version_info >= (3, 9, 0) and sys.version_info <= (3, 9, 1):\n",
        "\t\t# https://stackoverflow.com/questions/65858528/is-collections-abc-callable-bugged-in-python-3-9-1\n",
        "\t\tfrom typing import Callable\n",
        "\telse:\n",
        "\t\tfrom collections.abc import Callable\n",
        "\ttry:\n",
        "\t\tMeanFunc = Callable[[NDArray], float]\n",
        "\t\tHashFunc = Callable[[Image.Image], ImageHash]\n",
        "\texcept TypeError:\n",
        "\t\tMeanFunc = Callable  # type: ignore\n",
        "\t\tHashFunc = Callable  # type: ignore\n",
        "# end of dynamic code for typing\n",
        "\n",
        "\n",
        "def hex_to_hash(hexstr):\n",
        "\t# type: (str) -> ImageHash\n",
        "\t\"\"\"\n",
        "\tConvert a stored hash (hex, as retrieved from str(Imagehash))\n",
        "\tback to a Imagehash object.\n",
        "\n",
        "\tNotes:\n",
        "\t1. This algorithm assumes all hashes are either\n",
        "\t\t\tbidimensional arrays with dimensions hash_size * hash_size,\n",
        "\t\t\tor onedimensional arrays with dimensions binbits * 14.\n",
        "\t2. This algorithm does not work for hash_size < 2.\n",
        "\t\"\"\"\n",
        "\thash_size = int(numpy.sqrt(len(hexstr) * 4))\n",
        "\t# assert hash_size == numpy.sqrt(len(hexstr)*4)\n",
        "\tbinary_array = '{:0>{width}b}'.format(int(hexstr, 16), width=hash_size * hash_size)\n",
        "\tbit_rows = [binary_array[i:i + hash_size] for i in range(0, len(binary_array), hash_size)]\n",
        "\thash_array = numpy.array([[bool(int(d)) for d in row] for row in bit_rows])\n",
        "\treturn ImageHash(hash_array)\n",
        "\n",
        "\n",
        "def hex_to_flathash(hexstr, hashsize):\n",
        "\t# type: (str, int) -> ImageHash\n",
        "\thash_size = int(len(hexstr) * 4 / (hashsize))\n",
        "\tbinary_array = '{:0>{width}b}'.format(int(hexstr, 16), width=hash_size * hashsize)\n",
        "\thash_array = numpy.array([[bool(int(d)) for d in binary_array]])[-hash_size * hashsize:]\n",
        "\treturn ImageHash(hash_array)\n",
        "\n",
        "\n",
        "def hex_to_multihash(hexstr):\n",
        "\t# type: (str) -> ImageMultiHash\n",
        "\t\"\"\"\n",
        "\tConvert a stored multihash (hex, as retrieved from str(ImageMultiHash))\n",
        "\tback to an ImageMultiHash object.\n",
        "\n",
        "\tThis function is based on hex_to_hash so the same caveats apply. Namely:\n",
        "\n",
        "\t1. This algorithm assumes all hashes are either\n",
        "\t\t\tbidimensional arrays with dimensions hash_size * hash_size,\n",
        "\t\t\tor onedimensional arrays with dimensions binbits * 14.\n",
        "\t2. This algorithm does not work for hash_size < 2.\n",
        "\t\"\"\"\n",
        "\tsplit = hexstr.split(',')\n",
        "\thashes = [hex_to_hash(x) for x in split]\n",
        "\treturn ImageMultiHash(hashes)\n",
        "\n",
        "\n",
        "def old_hex_to_hash(hexstr, hash_size=8):\n",
        "\t# type: (str, int) -> ImageHash\n",
        "\t\"\"\"\n",
        "\tConvert a stored hash (hex, as retrieved from str(Imagehash))\n",
        "\tback to a Imagehash object. This method should be used for\n",
        "\thashes generated by ImageHash up to version 3.7. For hashes\n",
        "\tgenerated by newer versions of ImageHash, hex_to_hash should\n",
        "\tbe used instead.\n",
        "\t\"\"\"\n",
        "\tarr = []\n",
        "\tcount = hash_size * (hash_size // 4)\n",
        "\tif len(hexstr) != count:\n",
        "\t\temsg = 'Expected hex string size of {}.'\n",
        "\t\traise ValueError(emsg.format(count))\n",
        "\tfor i in range(count // 2):\n",
        "\t\th = hexstr[i * 2:i * 2 + 2]\n",
        "\t\tv = int('0x' + h, 16)\n",
        "\t\tarr.append([v & 2**i > 0 for i in range(8)])\n",
        "\treturn ImageHash(numpy.array(arr))\n",
        "\n",
        "\n",
        "def average_hash(image, hash_size=8, mean=numpy.mean):\n",
        "\t# type: (Image.Image, int, MeanFunc) -> ImageHash\n",
        "\t\"\"\"\n",
        "\tAverage Hash computation\n",
        "\n",
        "\tImplementation follows https://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html\n",
        "\n",
        "\tStep by step explanation: https://web.archive.org/web/20171112054354/https://www.safaribooksonline.com/blog/2013/11/26/image-hashing-with-python/ # noqa: E501\n",
        "\n",
        "\t@image must be a PIL instance.\n",
        "\t@mean how to determine the average luminescence. can try numpy.median instead.\n",
        "\t\"\"\"\n",
        "\tif hash_size < 2:\n",
        "\t\traise ValueError('Hash size must be greater than or equal to 2')\n",
        "\n",
        "\t# reduce size and complexity, then convert to grayscale\n",
        "\timage = image.convert('L').resize((hash_size, hash_size), ANTIALIAS)\n",
        "\n",
        "\t# find average pixel value; 'pixels' is an array of the pixel values, ranging from 0 (black) to 255 (white)\n",
        "\tpixels = numpy.asarray(image)\n",
        "\tavg = mean(pixels)\n",
        "\n",
        "\t# create string of bits\n",
        "\tdiff = pixels > avg\n",
        "\t# make a hash\n",
        "\treturn ImageHash(diff)\n",
        "\n",
        "\n",
        "def phash(image, hash_size=8, highfreq_factor=4):\n",
        "\t# type: (Image.Image, int, int) -> ImageHash\n",
        "\t\"\"\"\n",
        "\tPerceptual Hash computation.\n",
        "\n",
        "\tImplementation follows https://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html\n",
        "\n",
        "\t@image must be a PIL instance.\n",
        "\t\"\"\"\n",
        "\tif hash_size < 2:\n",
        "\t\traise ValueError('Hash size must be greater than or equal to 2')\n",
        "\n",
        "\timport scipy.fftpack\n",
        "\timg_size = hash_size * highfreq_factor\n",
        "\timage = image.convert('L').resize((img_size, img_size), ANTIALIAS)\n",
        "\tpixels = numpy.asarray(image)\n",
        "\tdct = scipy.fftpack.dct(scipy.fftpack.dct(pixels, axis=0), axis=1)\n",
        "\tdctlowfreq = dct[:hash_size, :hash_size]\n",
        "\tmed = numpy.median(dctlowfreq)\n",
        "\tdiff = dctlowfreq > med\n",
        "\treturn ImageHash(diff)\n",
        "\n",
        "\n",
        "def phash_simple(image, hash_size=8, highfreq_factor=4):\n",
        "\t# type: (Image.Image, int, int) -> ImageHash\n",
        "\t\"\"\"\n",
        "\tPerceptual Hash computation.\n",
        "\n",
        "\tImplementation follows https://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html\n",
        "\n",
        "\t@image must be a PIL instance.\n",
        "\t\"\"\"\n",
        "\timport scipy.fftpack\n",
        "\timg_size = hash_size * highfreq_factor\n",
        "\timage = image.convert('L').resize((img_size, img_size), ANTIALIAS)\n",
        "\tpixels = numpy.asarray(image)\n",
        "\tdct = scipy.fftpack.dct(pixels)\n",
        "\tdctlowfreq = dct[:hash_size, 1:hash_size + 1]\n",
        "\tavg = dctlowfreq.mean()\n",
        "\tdiff = dctlowfreq > avg\n",
        "\treturn ImageHash(diff)\n",
        "\n",
        "\n",
        "def dhash(image, hash_size=8):\n",
        "\t# type: (Image.Image, int) -> ImageHash\n",
        "\t\"\"\"\n",
        "\tDifference Hash computation.\n",
        "\n",
        "\tfollowing https://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html\n",
        "\n",
        "\tcomputes differences horizontally\n",
        "\n",
        "\t@image must be a PIL instance.\n",
        "\t\"\"\"\n",
        "\t# resize(w, h), but numpy.array((h, w))\n",
        "\tif hash_size < 2:\n",
        "\t\traise ValueError('Hash size must be greater than or equal to 2')\n",
        "\n",
        "\timage = image.convert('L').resize((hash_size + 1, hash_size), ANTIALIAS)\n",
        "\tpixels = numpy.asarray(image)\n",
        "\t# compute differences between columns\n",
        "\tdiff = pixels[:, 1:] > pixels[:, :-1]\n",
        "\treturn ImageHash(diff)\n",
        "\n",
        "\n",
        "def dhash_vertical(image, hash_size=8):\n",
        "\t# type: (Image.Image, int) -> ImageHash\n",
        "\t\"\"\"\n",
        "\tDifference Hash computation.\n",
        "\n",
        "\tfollowing https://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html\n",
        "\n",
        "\tcomputes differences vertically\n",
        "\n",
        "\t@image must be a PIL instance.\n",
        "\t\"\"\"\n",
        "\t# resize(w, h), but numpy.array((h, w))\n",
        "\timage = image.convert('L').resize((hash_size, hash_size + 1), ANTIALIAS)\n",
        "\tpixels = numpy.asarray(image)\n",
        "\t# compute differences between rows\n",
        "\tdiff = pixels[1:, :] > pixels[:-1, :]\n",
        "\treturn ImageHash(diff)\n",
        "\n",
        "\n",
        "def whash(image, hash_size=8, image_scale=None, mode='haar', remove_max_haar_ll=True):\n",
        "\t# type: (Image.Image, int, int | None, WhashMode, bool) -> ImageHash\n",
        "\t\"\"\"\n",
        "\tWavelet Hash computation.\n",
        "\n",
        "\tbased on https://www.kaggle.com/c/avito-duplicate-ads-detection/\n",
        "\n",
        "\t@image must be a PIL instance.\n",
        "\t@hash_size must be a power of 2 and less than @image_scale.\n",
        "\t@image_scale must be power of 2 and less than image size. By default is equal to max\n",
        "\t\t\t\t\tpower of 2 for an input image.\n",
        "\t@mode (see modes in pywt library):\n",
        "\t\t\t\t\t'haar' - Haar wavelets, by default\n",
        "\t\t\t\t\t'db4' - Daubechies wavelets\n",
        "\t@remove_max_haar_ll - remove the lowest low level (LL) frequency using Haar wavelet.\n",
        "\t\"\"\"\n",
        "\timport pywt\n",
        "\tif image_scale is not None:\n",
        "\t\tassert image_scale & (image_scale - 1) == 0, 'image_scale is not power of 2'\n",
        "\telse:\n",
        "\t\timage_natural_scale = 2**int(numpy.log2(min(image.size)))\n",
        "\t\timage_scale = max(image_natural_scale, hash_size)\n",
        "\n",
        "\tll_max_level = int(numpy.log2(image_scale))\n",
        "\n",
        "\tlevel = int(numpy.log2(hash_size))\n",
        "\tassert hash_size & (hash_size - 1) == 0, 'hash_size is not power of 2'\n",
        "\tassert level <= ll_max_level, 'hash_size in a wrong range'\n",
        "\tdwt_level = ll_max_level - level\n",
        "\n",
        "\timage = image.convert('L').resize((image_scale, image_scale), ANTIALIAS)\n",
        "\tpixels = numpy.asarray(image) / 255.\n",
        "\n",
        "\t# Remove low level frequency LL(max_ll) if @remove_max_haar_ll using haar filter\n",
        "\tif remove_max_haar_ll:\n",
        "\t\tcoeffs = pywt.wavedec2(pixels, 'haar', level=ll_max_level)\n",
        "\t\tcoeffs = list(coeffs)\n",
        "\t\tcoeffs[0] *= 0\n",
        "\t\tpixels = pywt.waverec2(coeffs, 'haar')\n",
        "\n",
        "\t# Use LL(K) as freq, where K is log2(@hash_size)\n",
        "\tcoeffs = pywt.wavedec2(pixels, mode, level=dwt_level)\n",
        "\tdwt_low = coeffs[0]\n",
        "\n",
        "\t# Subtract median and compute hash\n",
        "\tmed = numpy.median(dwt_low)\n",
        "\tdiff = dwt_low > med\n",
        "\treturn ImageHash(diff)\n",
        "\n",
        "\n",
        "def colorhash(image, binbits=3):\n",
        "\t# type: (Image.Image, int) -> ImageHash\n",
        "\t\"\"\"\n",
        "\tColor Hash computation.\n",
        "\n",
        "\tComputes fractions of image in intensity, hue and saturation bins:\n",
        "\n",
        "\t* the first binbits encode the black fraction of the image\n",
        "\t* the next binbits encode the gray fraction of the remaining image (low saturation)\n",
        "\t* the next 6*binbits encode the fraction in 6 bins of saturation, for highly saturated parts of the remaining image\n",
        "\t* the next 6*binbits encode the fraction in 6 bins of saturation, for mildly saturated parts of the remaining image\n",
        "\n",
        "\t@binbits number of bits to use to encode each pixel fractions\n",
        "\t\"\"\"\n",
        "\n",
        "\t# bin in hsv space:\n",
        "\tintensity = numpy.asarray(image.convert('L')).flatten()\n",
        "\th, s, v = [numpy.asarray(v).flatten() for v in image.convert('HSV').split()]\n",
        "\t# black bin\n",
        "\tmask_black = intensity < 256 // 8\n",
        "\tfrac_black = mask_black.mean()\n",
        "\t# gray bin (low saturation, but not black)\n",
        "\tmask_gray = s < 256 // 3\n",
        "\tfrac_gray = numpy.logical_and(~mask_black, mask_gray).mean()\n",
        "\t# two color bins (medium and high saturation, not in the two above)\n",
        "\tmask_colors = numpy.logical_and(~mask_black, ~mask_gray)\n",
        "\tmask_faint_colors = numpy.logical_and(mask_colors, s < 256 * 2 // 3)\n",
        "\tmask_bright_colors = numpy.logical_and(mask_colors, s > 256 * 2 // 3)\n",
        "\n",
        "\tc = max(1, mask_colors.sum())\n",
        "\t# in the color bins, make sub-bins by hue\n",
        "\thue_bins = numpy.linspace(0, 255, 6 + 1)\n",
        "\tif mask_faint_colors.any():\n",
        "\t\th_faint_counts, _ = numpy.histogram(h[mask_faint_colors], bins=hue_bins)\n",
        "\telse:\n",
        "\t\th_faint_counts = numpy.zeros(len(hue_bins) - 1)\n",
        "\tif mask_bright_colors.any():\n",
        "\t\th_bright_counts, _ = numpy.histogram(h[mask_bright_colors], bins=hue_bins)\n",
        "\telse:\n",
        "\t\th_bright_counts = numpy.zeros(len(hue_bins) - 1)\n",
        "\n",
        "\t# now we have fractions in each category (6*2 + 2 = 14 bins)\n",
        "\t# convert to hash and discretize:\n",
        "\tmaxvalue = 2**binbits\n",
        "\tvalues = [min(maxvalue - 1, int(frac_black * maxvalue)), min(maxvalue - 1, int(frac_gray * maxvalue))]\n",
        "\tfor counts in list(h_faint_counts) + list(h_bright_counts):\n",
        "\t\tvalues.append(min(maxvalue - 1, int(counts * maxvalue * 1. / c)))\n",
        "\t# print(values)\n",
        "\tbitarray = []\n",
        "\tfor v in values:\n",
        "\t\tbitarray += [v // (2**(binbits - i - 1)) % 2**(binbits - i) > 0 for i in range(binbits)]\n",
        "\treturn ImageHash(numpy.asarray(bitarray).reshape((-1, binbits)))\n",
        "\n",
        "\n",
        "class ImageMultiHash:\n",
        "\t\"\"\"\n",
        "\tThis is an image hash containing a list of individual hashes for segments of the image.\n",
        "\tThe matching logic is implemented as described in Efficient Cropping-Resistant Robust Image Hashing\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, hashes):\n",
        "\t\t# type: (list[ImageHash]) -> None\n",
        "\t\tself.segment_hashes = hashes  # type: list[ImageHash]\n",
        "\n",
        "\tdef __eq__(self, other):\n",
        "\t\t# type: (object) -> bool\n",
        "\t\tif other is None:\n",
        "\t\t\treturn False\n",
        "\t\treturn self.matches(other)  # type: ignore\n",
        "\n",
        "\tdef __ne__(self, other):\n",
        "\t\t# type: (object) -> bool\n",
        "\t\treturn not self.matches(other)  # type: ignore\n",
        "\n",
        "\tdef __sub__(self, other, hamming_cutoff=None, bit_error_rate=None):\n",
        "\t\t# type: (ImageMultiHash, float | None, float | None) -> float\n",
        "\t\tmatches, sum_distance = self.hash_diff(other, hamming_cutoff, bit_error_rate)\n",
        "\t\tmax_difference = len(self.segment_hashes)\n",
        "\t\tif matches == 0:\n",
        "\t\t\treturn max_difference\n",
        "\t\tmax_distance = matches * len(self.segment_hashes[0])\n",
        "\t\ttie_breaker = 0 - (float(sum_distance) / max_distance)\n",
        "\t\tmatch_score = matches + tie_breaker\n",
        "\t\treturn max_difference - match_score\n",
        "\n",
        "\tdef __hash__(self):\n",
        "\t\treturn hash(tuple(hash(segment) for segment in self.segment_hashes))\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\treturn ','.join(str(x) for x in self.segment_hashes)\n",
        "\n",
        "\tdef __repr__(self):\n",
        "\t\treturn repr(self.segment_hashes)\n",
        "\n",
        "\tdef hash_diff(self, other_hash, hamming_cutoff=None, bit_error_rate=None):\n",
        "\t\t# type: (ImageMultiHash, float | None, float | None) -> tuple[int, int]\n",
        "\t\t\"\"\"\n",
        "\t\tGets the difference between two multi-hashes, as a tuple. The first element of the tuple is the number of\n",
        "\t\tmatching segments, and the second element is the sum of the hamming distances of matching hashes.\n",
        "\t\tNOTE: Do not order directly by this tuple, as higher is better for matches, and worse for hamming cutoff.\n",
        "\t\t:param other_hash: The image multi hash to compare against\n",
        "\t\t:param hamming_cutoff: The maximum hamming distance to a region hash in the target hash\n",
        "\t\t:param bit_error_rate: Percentage of bits which can be incorrect, an alternative to the hamming cutoff. The\n",
        "\t\tdefault of 0.25 means that the segment hashes can be up to 25% different\n",
        "\t\t\"\"\"\n",
        "\t\t# Set default hamming cutoff if it's not set.\n",
        "\t\tif hamming_cutoff is None:\n",
        "\t\t\tif bit_error_rate is None:\n",
        "\t\t\t\tbit_error_rate = 0.25\n",
        "\t\t\thamming_cutoff = len(self.segment_hashes[0]) * bit_error_rate\n",
        "\t\t# Get the hash distance for each region hash within cutoff\n",
        "\t\tdistances = []\n",
        "\t\tfor segment_hash in self.segment_hashes:\n",
        "\t\t\tlowest_distance = min(\n",
        "\t\t\t\tsegment_hash - other_segment_hash\n",
        "\t\t\t\tfor other_segment_hash in other_hash.segment_hashes\n",
        "\t\t\t)\n",
        "\t\t\tif lowest_distance > hamming_cutoff:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tdistances.append(lowest_distance)\n",
        "\t\treturn len(distances), sum(distances)\n",
        "\n",
        "\tdef matches(self, other_hash, region_cutoff=1, hamming_cutoff=None, bit_error_rate=None):\n",
        "\t\t# type: (ImageMultiHash, int, float | None, float | None) -> bool\n",
        "\t\t\"\"\"\n",
        "\t\tChecks whether this hash matches another crop resistant hash, `other_hash`.\n",
        "\t\t:param other_hash: The image multi hash to compare against\n",
        "\t\t:param region_cutoff: The minimum number of regions which must have a matching hash\n",
        "\t\t:param hamming_cutoff: The maximum hamming distance to a region hash in the target hash\n",
        "\t\t:param bit_error_rate: Percentage of bits which can be incorrect, an alternative to the hamming cutoff. The\n",
        "\t\tdefault of 0.25 means that the segment hashes can be up to 25% different\n",
        "\t\t\"\"\"\n",
        "\t\tmatches, _ = self.hash_diff(other_hash, hamming_cutoff, bit_error_rate)\n",
        "\t\treturn matches >= region_cutoff\n",
        "\n",
        "\tdef best_match(self, other_hashes, hamming_cutoff=None, bit_error_rate=None):\n",
        "\t\t# type: (list[ImageMultiHash], float | None, float | None) -> ImageMultiHash\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the hash in a list which is the best match to the current hash\n",
        "\t\t:param other_hashes: A list of image multi hashes to compare against\n",
        "\t\t:param hamming_cutoff: The maximum hamming distance to a region hash in the target hash\n",
        "\t\t:param bit_error_rate: Percentage of bits which can be incorrect, an alternative to the hamming cutoff.\n",
        "\t\tDefaults to 0.25 if unset, which means the hash can be 25% different\n",
        "\t\t\"\"\"\n",
        "\t\treturn min(\n",
        "\t\t\tother_hashes,\n",
        "\t\t\tkey=lambda other_hash: self.__sub__(other_hash, hamming_cutoff, bit_error_rate)\n",
        "\t\t)\n",
        "\n",
        "\n",
        "def _find_region(remaining_pixels, segmented_pixels):\n",
        "\t\"\"\"\n",
        "\tFinds a region and returns a set of pixel coordinates for it.\n",
        "\t:param remaining_pixels: A numpy bool array, with True meaning the pixels are remaining to segment\n",
        "\t:param segmented_pixels: A set of pixel coordinates which have already been assigned to segment. This will be\n",
        "\tupdated with the new pixels added to the returned segment.\n",
        "\t\"\"\"\n",
        "\tin_region = set()\n",
        "\tnot_in_region = set()\n",
        "\t# Find the first pixel in remaining_pixels with a value of True\n",
        "\tavailable_pixels = numpy.transpose(numpy.nonzero(remaining_pixels))\n",
        "\tstart = tuple(available_pixels[0])\n",
        "\tin_region.add(start)\n",
        "\tnew_pixels = in_region.copy()\n",
        "\twhile True:\n",
        "\t\ttry_next = set()\n",
        "\t\t# Find surrounding pixels\n",
        "\t\tfor pixel in new_pixels:\n",
        "\t\t\tx, y = pixel\n",
        "\t\t\tneighbours = [\n",
        "\t\t\t\t(x - 1, y),\n",
        "\t\t\t\t(x + 1, y),\n",
        "\t\t\t\t(x, y - 1),\n",
        "\t\t\t\t(x, y + 1)\n",
        "\t\t\t]\n",
        "\t\t\ttry_next.update(neighbours)\n",
        "\t\t# Remove pixels we have already seen\n",
        "\t\ttry_next.difference_update(segmented_pixels, not_in_region)\n",
        "\t\t# If there's no more pixels to try, the region is complete\n",
        "\t\tif not try_next:\n",
        "\t\t\tbreak\n",
        "\t\t# Empty new pixels set, so we know whose neighbour's to check next time\n",
        "\t\tnew_pixels = set()\n",
        "\t\t# Check new pixels\n",
        "\t\tfor pixel in try_next:\n",
        "\t\t\tif remaining_pixels[pixel]:\n",
        "\t\t\t\tin_region.add(pixel)\n",
        "\t\t\t\tnew_pixels.add(pixel)\n",
        "\t\t\t\tsegmented_pixels.add(pixel)\n",
        "\t\t\telse:\n",
        "\t\t\t\tnot_in_region.add(pixel)\n",
        "\treturn in_region\n",
        "\n",
        "\n",
        "def _find_all_segments(pixels, segment_threshold, min_segment_size):\n",
        "\t\"\"\"\n",
        "\tFinds all the regions within an image pixel array, and returns a list of the regions.\n",
        "\n",
        "\tNote: Slightly different segmentations are produced when using pillow version 6 vs. >=7, due to a change in\n",
        "\trounding in the greyscale conversion.\n",
        "\t:param pixels: A numpy array of the pixel brightnesses.\n",
        "\t:param segment_threshold: The brightness threshold to use when differentiating between hills and valleys.\n",
        "\t:param min_segment_size: The minimum number of pixels for a segment.\n",
        "\t\"\"\"\n",
        "\timg_width, img_height = pixels.shape\n",
        "\t# threshold pixels\n",
        "\tthreshold_pixels = pixels > segment_threshold\n",
        "\tunassigned_pixels = numpy.full(pixels.shape, True, dtype=bool)\n",
        "\n",
        "\tsegments = []\n",
        "\talready_segmented = set()\n",
        "\n",
        "\t# Add all the pixels around the border outside the image:\n",
        "\talready_segmented.update([(-1, z) for z in range(img_height)])\n",
        "\talready_segmented.update([(z, -1) for z in range(img_width)])\n",
        "\talready_segmented.update([(img_width, z) for z in range(img_height)])\n",
        "\talready_segmented.update([(z, img_height) for z in range(img_width)])\n",
        "\n",
        "\t# Find all the \"hill\" regions\n",
        "\twhile numpy.bitwise_and(threshold_pixels, unassigned_pixels).any():\n",
        "\t\tremaining_pixels = numpy.bitwise_and(threshold_pixels, unassigned_pixels)\n",
        "\t\tsegment = _find_region(remaining_pixels, already_segmented)\n",
        "\t\t# Apply segment\n",
        "\t\tif len(segment) > min_segment_size:\n",
        "\t\t\tsegments.append(segment)\n",
        "\t\tfor pix in segment:\n",
        "\t\t\tunassigned_pixels[pix] = False\n",
        "\n",
        "\t# Invert the threshold matrix, and find \"valleys\"\n",
        "\tthreshold_pixels_i = numpy.invert(threshold_pixels)\n",
        "\twhile len(already_segmented) < img_width * img_height:\n",
        "\t\tremaining_pixels = numpy.bitwise_and(threshold_pixels_i, unassigned_pixels)\n",
        "\t\tsegment = _find_region(remaining_pixels, already_segmented)\n",
        "\t\t# Apply segment\n",
        "\t\tif len(segment) > min_segment_size:\n",
        "\t\t\tsegments.append(segment)\n",
        "\t\tfor pix in segment:\n",
        "\t\t\tunassigned_pixels[pix] = False\n",
        "\n",
        "\treturn segments\n",
        "\n",
        "\n",
        "def crop_resistant_hash(\n",
        "\timage,  # type: Image.Image\n",
        "\thash_func=None,  # type: HashFunc\n",
        "\tlimit_segments=None,  # type: int | None\n",
        "\tsegment_threshold=128,  # type: int\n",
        "\tmin_segment_size=500,  # type: int\n",
        "\tsegmentation_image_size=300  # type: int\n",
        "):\n",
        "\t# type: (...) -> ImageMultiHash\n",
        "\t\"\"\"\n",
        "\tCreates a CropResistantHash object, by the algorithm described in the paper \"Efficient Cropping-Resistant Robust\n",
        "\tImage Hashing\". DOI 10.1109/ARES.2014.85\n",
        "\tThis algorithm partitions the image into bright and dark segments, using a watershed-like algorithm, and then does\n",
        "\tan image hash on each segment. This makes the image much more resistant to cropping than other algorithms, with\n",
        "\tthe paper claiming resistance to up to 50% cropping, while most other algorithms stop at about 5% cropping.\n",
        "\n",
        "\tNote: Slightly different segmentations are produced when using pillow version 6 vs. >=7, due to a change in\n",
        "\trounding in the greyscale conversion. This leads to a slightly different result.\n",
        "\t:param image: The image to hash\n",
        "\t:param hash_func: The hashing function to use\n",
        "\t:param limit_segments: If you have storage requirements, you can limit to hashing only the M largest segments\n",
        "\t:param segment_threshold: Brightness threshold between hills and valleys. This should be static, putting it between\n",
        "\tpeak and through dynamically breaks the matching\n",
        "\t:param min_segment_size: Minimum number of pixels for a hashable segment\n",
        "\t:param segmentation_image_size: Size which the image is resized to before segmentation\n",
        "\t\"\"\"\n",
        "\tif hash_func is None:\n",
        "\t\thash_func = dhash\n",
        "\n",
        "\torig_image = image.copy()\n",
        "\t# Convert to gray scale and resize\n",
        "\timage = image.convert('L').resize((segmentation_image_size, segmentation_image_size), ANTIALIAS)\n",
        "\t# Add filters\n",
        "\timage = image.filter(ImageFilter.GaussianBlur()).filter(ImageFilter.MedianFilter())\n",
        "\tpixels = numpy.array(image).astype(numpy.float32)\n",
        "\n",
        "\tsegments = _find_all_segments(pixels, segment_threshold, min_segment_size)\n",
        "\n",
        "\t# If there are no segments, have 1 segment including the whole image\n",
        "\tif not segments:\n",
        "\t\tfull_image_segment = {(0, 0), (segmentation_image_size - 1, segmentation_image_size - 1)}\n",
        "\t\tsegments.append(full_image_segment)\n",
        "\n",
        "\t# If segment limit is set, discard the smaller segments\n",
        "\tif limit_segments:\n",
        "\t\tsegments = sorted(segments, key=lambda s: len(s), reverse=True)[:limit_segments]\n",
        "\n",
        "\t# Create bounding box for each segment\n",
        "\thashes = []\n",
        "\tfor segment in segments:\n",
        "\t\torig_w, orig_h = orig_image.size\n",
        "\t\tscale_w = float(orig_w) / segmentation_image_size\n",
        "\t\tscale_h = float(orig_h) / segmentation_image_size\n",
        "\t\tmin_y = min(coord[0] for coord in segment) * scale_h\n",
        "\t\tmin_x = min(coord[1] for coord in segment) * scale_w\n",
        "\t\tmax_y = (max(coord[0] for coord in segment) + 1) * scale_h\n",
        "\t\tmax_x = (max(coord[1] for coord in segment) + 1) * scale_w\n",
        "\t\t# Compute robust hash for each bounding box\n",
        "\t\tbounding_box = orig_image.crop((min_x, min_y, max_x, max_y))\n",
        "\t\thashes.append(hash_func(bounding_box))\n",
        "\t\t# Show bounding box\n",
        "\t\t# im_segment = image.copy()\n",
        "\t\t# for pix in segment:\n",
        "\t\t# \tim_segment.putpixel(pix[::-1], 255)\n",
        "\t\t# im_segment.show()\n",
        "\t\t# bounding_box.show()\n",
        "\n",
        "\treturn ImageMultiHash(hashes)"
      ],
      "metadata": {
        "id": "oLbuidiC72y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import imagehash\n",
        "hash = imagehash.average_hash(Image.open('1.jpeg'))\n",
        "print(hash)\n",
        "otherhash = imagehash.average_hash(Image.open('/content/WhatsApp Image 2024-04-20 at 1.43.37 AM.jpeg'))\n",
        "print(otherhash)\n",
        "print(hash == otherhash)\n",
        "print(hash - otherhash)  # hamming distance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJSHL2Zn8uM0",
        "outputId": "2d332a70-5f4d-4bc5-b2a7-b044de8aed42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0000000000000000\n",
            "0000181818180000\n",
            "False\n",
            "8\n"
          ]
        }
      ]
    }
  ]
}