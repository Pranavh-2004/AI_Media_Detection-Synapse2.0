{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpVX/AmTklFef6BQITzHRw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pranavh-2004/AI_Media_Detection-Synapse2.0/blob/main/objectcoords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import zipfile\n",
        "import os\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\n",
        "zipref=zipfile.ZipFile(\"10_food_classes_all_data.zip\")\n",
        "zipref.extractall()\n",
        "zipref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyV12nYsGJdK",
        "outputId": "f4cd3cb2-e231-4225-e977-d5895e33558c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-19 20:54:01--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.10.207, 142.251.12.207, 172.217.194.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.10.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 519183241 (495M) [application/zip]\n",
            "Saving to: ‘10_food_classes_all_data.zip’\n",
            "\n",
            "10_food_classes_all 100%[===================>] 495.13M  22.4MB/s    in 25s     \n",
            "\n",
            "2024-04-19 20:54:26 (20.2 MB/s) - ‘10_food_classes_all_data.zip’ saved [519183241/519183241]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Define ImageDataGenerators for training and testing\n",
        "\n",
        "train_datagen_augmented = ImageDataGenerator(\n",
        "    rotation_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    height_shift_range=0.2,\n",
        "    width_shift_range=0.2\n",
        "\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    #rescale=1./255\n",
        "\n",
        ")\n",
        "train_datagen = ImageDataGenerator(\n",
        "    #rescale=1./255\n",
        "\n",
        ")\n",
        "\n",
        "# Flow training images in batches using the generator\n",
        "train_data_augmented = train_datagen_augmented.flow_from_directory(\n",
        "    '10_food_classes_all_data/train/',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    seed=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Flow validation images in batches using the generator\n",
        "test_data = test_datagen.flow_from_directory(\n",
        "    '10_food_classes_all_data/test/',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    seed=42\n",
        ")\n",
        "train_data=train_datagen.flow_from_directory(\n",
        "    '10_food_classes_all_data/train/',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    seed=42,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEOBFDDmGWxY",
        "outputId": "4322f5e3-8034-403f-d127-e21384fac786"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7500 images belonging to 10 classes.\n",
            "Found 2500 images belonging to 10 classes.\n",
            "Found 7500 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "base_model=tf.keras.applications.EfficientNetB4(include_top=False)\n",
        "base_model.trainable=False #we are taking pre trained model so we shouldnt train it and disturb its accuracy\n",
        "\n",
        "inputs=tf.keras.layers.Input(shape=(128,128,3))\n",
        "x=base_model(inputs)\n",
        "x=tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "outputs=tf.keras.layers.Dense(10,activation='softmax')(x)\n",
        "model=tf.keras.models.Model(inputs,outputs)\n",
        "model.summary()\n",
        "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "               optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "               metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAuUfYpTGIYJ",
        "outputId": "66ca6352-4e89-48c9-c6b0-1b57ca945798"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
            "71686520/71686520 [==============================] - 4s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
            "                                                                 \n",
            " efficientnetb4 (Functional  (None, None, None, 1792   17673823  \n",
            " )                           )                                   \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 1792)              0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                17930     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17691753 (67.49 MB)\n",
            "Trainable params: 17930 (70.04 KB)\n",
            "Non-trainable params: 17673823 (67.42 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_data_augmented,epochs=3,validation_data=test_data,steps_per_epoch=len(train_data),validation_steps=int(0.2*len(test_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIobr5UOH9em",
        "outputId": "53bcbe0d-433b-4e92-96b6-48ad908371b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "235/235 [==============================] - 77s 252ms/step - loss: 1.2630 - accuracy: 0.6052 - val_loss: 0.6792 - val_accuracy: 0.7875\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 56s 239ms/step - loss: 0.9388 - accuracy: 0.6980 - val_loss: 0.6767 - val_accuracy: 0.8042\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 53s 225ms/step - loss: 0.8767 - accuracy: 0.7176 - val_loss: 0.5782 - val_accuracy: 0.8271\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b81116967d0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable=True\n",
        "for i,layer in enumerate(base_model.layers[:-10]):\n",
        "  layer.trainable=False"
      ],
      "metadata": {
        "id": "eoce1KJCGgxq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "               optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),#new learning rate after changing layers=old learning rate/10\n",
        "               metrics=['accuracy'])\n",
        "model.fit(train_data_augmented,epochs=6,validation_data=test_data,steps_per_epoch=len(train_data),validation_steps=int(0.2*len(test_data)),initial_epoch=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SblfVEe1GkbJ",
        "outputId": "40f7fa28-592b-452a-ee6a-5abfa88d204f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/6\n",
            "235/235 [==============================] - 76s 255ms/step - loss: 0.8664 - accuracy: 0.7171 - val_loss: 0.6233 - val_accuracy: 0.8000\n",
            "Epoch 5/6\n",
            "235/235 [==============================] - 55s 233ms/step - loss: 0.7746 - accuracy: 0.7456 - val_loss: 0.5537 - val_accuracy: 0.8125\n",
            "Epoch 6/6\n",
            "235/235 [==============================] - 54s 228ms/step - loss: 0.7129 - accuracy: 0.7620 - val_loss: 0.5278 - val_accuracy: 0.8292\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b817034ab90>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform inference on sample images from the training data\n",
        "images, labels = next(train_data)  # Get one batch for inference\n",
        "predictions = model.predict(images)\n",
        "\n",
        "# Convert predictions to desired format\n",
        "object_positions_list = []\n",
        "for pred in predictions:\n",
        "    object_positions = []\n",
        "    for i in range(0, len(pred), 4):  # Assuming each object has 4 coordinates (a, b, c, d)\n",
        "        if i + 4 <= len(pred):  # Check if there are enough values for unpacking\n",
        "            a, b, c, d = pred[i:i+4]\n",
        "            object_positions.append({'x': [[a, b], [c, d]]})\n",
        "        else:\n",
        "            break\n",
        "    object_positions_list.append({'objects': object_positions})\n",
        "\n",
        "print(object_positions_list)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c68JR6jgIGnG",
        "outputId": "7fa420b6-5d9e-491d-c79f-a2cbc2f86ffe"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 35ms/step\n",
            "[{'objects': [{'x': [[0.033731963, 0.0051096044], [0.022005234, 0.07274693]]}, {'x': [[0.00475481, 0.0014410185], [0.008701401, 0.0006028445]]}]}, {'objects': [{'x': [[2.918629e-05, 0.0007789364], [3.8041452e-07, 0.98972964]]}, {'x': [[7.968514e-05, 4.0096547e-06], [1.8771818e-05, 6.3805796e-06]]}]}, {'objects': [{'x': [[0.009953657, 0.00010998482], [0.0014655878, 0.008566509]]}, {'x': [[0.00012269623, 0.00022677687], [2.0408634e-05, 0.96875584]]}]}, {'objects': [{'x': [[0.00055645505, 3.8104907e-05], [0.0017220118, 8.031161e-05]]}, {'x': [[5.288905e-05, 0.99663407], [5.312984e-05, 0.0007759536]]}]}, {'objects': [{'x': [[0.015831172, 0.0006833043], [0.0015981463, 0.00039199868]]}, {'x': [[0.0003311118, 0.0001388244], [0.00093891314, 0.9769976]]}]}, {'objects': [{'x': [[0.0029412874, 0.0005573219], [0.00021024534, 0.0004007539]]}, {'x': [[0.000311024, 0.0013752559], [8.548732e-05, 4.678985e-06]]}]}, {'objects': [{'x': [[0.4002878, 0.028069954], [0.0048285625, 0.058433734]]}, {'x': [[0.36363596, 0.11086711], [0.0015530154, 0.0057500307]]}]}, {'objects': [{'x': [[0.0006307912, 0.0003515202], [0.007883075, 1.7411596e-05]]}, {'x': [[1.4269285e-05, 0.99048775], [0.00037796973, 5.9853308e-05]]}]}, {'objects': [{'x': [[0.0001103534, 0.99556434], [9.545287e-06, 0.00090203725]]}, {'x': [[0.00043276735, 1.6076689e-06], [3.1350442e-06, 2.4563107e-05]]}]}, {'objects': [{'x': [[0.03516836, 0.00017626882], [0.92808515, 0.01199291]]}, {'x': [[0.00071350276, 0.00017745148], [0.001013985, 0.02053805]]}]}, {'objects': [{'x': [[0.23021868, 0.05898713], [0.061760746, 0.095756866]]}, {'x': [[0.036829352, 0.04059208], [0.037883885, 0.0844649]]}]}, {'objects': [{'x': [[0.010323772, 4.8990816e-05], [0.98512197, 0.0010519953]]}, {'x': [[0.00011494692, 0.0007721699], [0.0007109472, 0.00092498504]]}]}, {'objects': [{'x': [[0.12107421, 0.037313852], [0.00818233, 0.72211015]]}, {'x': [[0.011204683, 0.0061370865], [0.0011240125, 0.0018331648]]}]}, {'objects': [{'x': [[0.00036708606, 0.9938496], [0.000109256835, 0.0036042589]]}, {'x': [[0.0006469717, 4.97516e-06], [8.528433e-06, 1.824784e-05]]}]}, {'objects': [{'x': [[0.0019225858, 0.9674448], [5.4718726e-05, 0.027746297]]}, {'x': [[0.0007224136, 3.6932957e-05], [0.00013420077, 4.3034623e-05]]}]}, {'objects': [{'x': [[0.050373994, 0.0056726243], [0.008237905, 0.011291097]]}, {'x': [[0.0019699282, 8.187605e-05], [0.0005457651, 0.91869456]]}]}, {'objects': [{'x': [[4.267963e-05, 4.9701775e-05], [2.9636429e-05, 2.7493277e-05]]}, {'x': [[8.561929e-05, 3.6828682e-05], [0.999584, 7.029819e-05]]}]}, {'objects': [{'x': [[6.223509e-05, 3.4425902e-05], [0.00016544649, 2.7232978e-05]]}, {'x': [[1.8817625e-05, 0.9981171], [0.00051625737, 9.195233e-05]]}]}, {'objects': [{'x': [[0.4707443, 0.07341766], [0.072466224, 0.043320887]]}, {'x': [[0.09305176, 0.006581955], [0.033050634, 0.18645817]]}]}, {'objects': [{'x': [[0.00065964524, 0.99575555], [2.0091316e-05, 0.001289941]]}, {'x': [[0.0007654242, 4.2342002e-05], [6.587312e-05, 8.868526e-07]]}]}, {'objects': [{'x': [[0.996064, 0.0010365875], [7.5150936e-05, 0.000106522726]]}, {'x': [[0.0015922202, 0.00021804849], [1.9625766e-05, 0.00019328529]]}]}, {'objects': [{'x': [[0.0012967882, 0.9917824], [8.8044195e-05, 0.004861368]]}, {'x': [[0.0009987182, 0.0003985527], [0.00018718583, 1.0596135e-05]]}]}, {'objects': [{'x': [[4.4792447e-05, 7.096825e-06], [6.0768652e-05, 0.000105889376]]}, {'x': [[0.0011347679, 0.99857867], [8.097829e-06, 2.320296e-05]]}]}, {'objects': [{'x': [[0.00031037105, 0.011971094], [0.00018288664, 0.15776351]]}, {'x': [[0.0004033626, 1.2644137e-05], [7.76109e-05, 2.5870359e-05]]}]}, {'objects': [{'x': [[0.09050056, 0.4173338], [0.0052508125, 0.16516314]]}, {'x': [[0.028327137, 0.024611726], [0.010844829, 0.023944763]]}]}, {'objects': [{'x': [[0.0004481039, 0.00023645304], [0.005154021, 0.00050142896]]}, {'x': [[0.0005588805, 0.98014313], [0.0037024533, 0.00063871645]]}]}, {'objects': [{'x': [[0.0007735073, 9.56529e-06], [0.00021476622, 4.4638244e-05]]}, {'x': [[6.0983725e-06, 0.00021781362], [4.301921e-05, 0.9983968]]}]}, {'objects': [{'x': [[0.0013937809, 7.112109e-05], [0.004489414, 0.0006932718]]}, {'x': [[5.5546112e-05, 0.0001811752], [9.47132e-05, 0.9916569]]}]}, {'objects': [{'x': [[7.7902274e-05, 0.00015905358], [0.0002607465, 0.00054828]]}, {'x': [[0.99782974, 0.00023152116], [7.0055336e-05, 0.00012775464]]}]}, {'objects': [{'x': [[0.06065021, 0.22733973], [0.0140402755, 0.08944777]]}, {'x': [[0.39458367, 0.0004588546], [0.0018788903, 0.002411855]]}]}, {'objects': [{'x': [[0.52820224, 0.3687654], [0.006856127, 0.0035770892]]}, {'x': [[0.020007895, 0.004923463], [0.0153733995, 0.0044008708]]}]}, {'objects': [{'x': [[0.0027943652, 0.001529272], [8.928049e-05, 0.002180445]]}, {'x': [[0.002085499, 0.00025021165], [5.5141038e-05, 3.932249e-05]]}]}]\n"
          ]
        }
      ]
    }
  ]
}